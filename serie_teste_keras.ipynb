{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from pyspark.sql import SparkSession\n",
    "import findspark\n",
    "import os\n",
    "import plotly.express as px\n",
    "from transformers import TFAutoModel\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from pyspark.ml import Pipeline\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import ChiSqSelector\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.fpm import FPGrowth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=r\"D:\\estudos_0012708\\dados_super_store.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"SPARK_HOME\"]=r\"D:\\estudos_0012708\\venv\\Lib\\site-packages\\pyspark\"\n",
    "os.environ[\"HADOOP_HOME\"]=r\"D:\\hadoop-3.3.6\"\n",
    "os.environ[\"JAVA_HOME\"]=r\"C:\\Program Files\\Java\\jdk-20\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master('local[*]') \\\n",
    "    .appName(\"analise_nlp\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_line_plot(data, x,y, name):\n",
    "    fig = px.line(data, x=x, y=y, title=name)\n",
    "\n",
    "\n",
    "    fig.update_layout(\n",
    "        paper_bgcolor='rgba(0,0,0,0)',  \n",
    "        plot_bgcolor='rgba(0,0,0,0)',   \n",
    "        font_color='white',            \n",
    "    )\n",
    "    fig.update_traces(line=dict(color='rgb(148, 0, 211)'))  \n",
    "\n",
    "    fig.update_layout(hovermode='x')\n",
    "\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def replace_null_with_zero(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        result_df = func(*args, **kwargs)\n",
    "        columns = result_df.columns\n",
    "       \n",
    "        result_df = result_df.select(\n",
    "            *[F.when(F.col(col).isNull(), 0).otherwise(F.col(col)).alias(col) for col in columns]\n",
    "        )\n",
    "        return result_df\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "\n",
    "def select_best_features(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        df = func(*args, **kwargs)\n",
    "       \n",
    "        feature_cols = ['valor', 'order', 'clienteid'] \n",
    "  \n",
    "        vector_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "       \n",
    "        df = vector_assembler.transform(df)\n",
    "    \n",
    "        selector = ChiSqSelector(numTopFeatures=5, featuresCol=\"features\", outputCol=\"selectedFeatures\", labelCol=\"target\")\n",
    "        model = selector.fit(df)\n",
    "        df = model.transform(df)\n",
    "        return df\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "\n",
    "def convert_categorical_to_onehot(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        result_df = func(*args, **kwargs)\n",
    "        columns = result_df.columns\n",
    "      \n",
    "        for col in columns:\n",
    "            data_type = result_df.schema[col].dataType\n",
    "            if isinstance(data_type, StringType): \n",
    "                encoder = OneHotEncoder(inputCol=col, outputCol=f\"{col}_onehot\")\n",
    "                result_df = encoder.transform(result_df)\n",
    "        return result_df\n",
    "    return wrapper\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def optimize_fpgrowth(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        df = func(*args, **kwargs)\n",
    "        \n",
    "      \n",
    "        train_data, test_data = df.randomSplit([0.8, 0.2], seed=123)\n",
    "        \n",
    "        fp_growth = FPGrowth(itemsCol=\"items\", minSupport=0.1, minConfidence=0.2)\n",
    "        \n",
    "     \n",
    "        param_grid = ParamGridBuilder() \\\n",
    "            .addGrid(fp_growth.minSupport, [0.05, 0.1, 0.2]) \\\n",
    "            .addGrid(fp_growth.minConfidence, [0.1, 0.2, 0.3]) \\\n",
    "            .build()\n",
    "        \n",
    "        \n",
    "        crossval = CrossValidator(estimator=fp_growth,\n",
    "                                  estimatorParamMaps=param_grid,\n",
    "                                  evaluator=None, \n",
    "                                  numFolds=5)\n",
    "        \n",
    "      \n",
    "        model = crossval.fit(train_data)\n",
    "        \n",
    "        return model\n",
    "    return wrapper\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "@optimize_fpgrowth\n",
    "def read_data_and_optimize_fpgrowth(file_path):\n",
    "    df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "    df = df.groupBy(\"transaction_id\").agg(expr(\"collect_list(item_id) as items\"))\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "optimized_fpgrowth_model = read_data_and_optimize_fpgrowth(file_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def optimize_als(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        df = func(*args, **kwargs)\n",
    "        \n",
    "      \n",
    "        train_data, test_data = df.randomSplit([0.8, 0.2], seed=123)\n",
    "        \n",
    "     \n",
    "        als = ALS(userCol=\"user\", itemCol=\"item\", ratingCol=\"rating\", coldStartStrategy=\"drop\")\n",
    "        \n",
    "      \n",
    "        param_grid = ParamGridBuilder() \\\n",
    "            .addGrid(als.rank, [10, 20, 30]) \\\n",
    "            .addGrid(als.maxIter, [10, 20, 30]) \\\n",
    "            .addGrid(als.regParam, [0.01, 0.1, 0.2]) \\\n",
    "            .build()\n",
    "        \n",
    "     \n",
    "        evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "        \n",
    "       \n",
    "        crossval = CrossValidator(estimator=als,\n",
    "                                  estimatorParamMaps=param_grid,\n",
    "                                  evaluator=evaluator,\n",
    "                                  numFolds=5)\n",
    "        \n",
    "        model = crossval.fit(train_data)\n",
    "        \n",
    " \n",
    "        predictions = model.transform(test_data)\n",
    "        rmse = evaluator.evaluate(predictions)\n",
    "        print(f\"RMSE no conjunto de teste: {rmse:.2f}\")\n",
    "        \n",
    "        return model\n",
    "    return wrapper\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@optimize_als\n",
    "def read_data_and_optimize_als(file_path):\n",
    "    df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "    df = df.selectExpr(\"userId as user\", \"movieId as item\", \"rating\")\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "optimized_als_model = read_data_and_optimize_als(file_path)\n",
    "\n",
    "\n",
    "\n",
    "@convert_categorical_to_onehot\n",
    "def read_csv_and_convert_to_onehot(file_path):\n",
    "    df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "df_with_onehot = read_csv_and_convert_to_onehot(file_path)\n",
    "\n",
    "\n",
    "@replace_null_with_zero\n",
    "def read_csv_with_null_replacement(file_path):\n",
    "    df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "df_with_replacement = read_csv_with_null_replacement(file_path)\n",
    "\n",
    "\n",
    "\n",
    "@select_best_features\n",
    "def read_csv_and_select_features(file_path):\n",
    "    df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "df_selected_features = read_csv_and_select_features(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reader_csv_files(csvfiles):\n",
    "    file_path=csvfiles\n",
    "\n",
    "    df = spark.read.csv(file_path, header=True, inferSchema=True,sep=\";\")\n",
    "    df.columns\n",
    "    \n",
    "file_path=r\"D:\\estudos_0012708\\dados_super_store.csv\"  \n",
    "reader_csv_files(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_sales_and_profit(df):\n",
    "    total_sales = df.groupBy(\"Category\").sum(\"Sales\").alias(\"TotalSales\")\n",
    "    avg_profit = df.groupBy(\"Category\").avg(\"Profit\").alias(\"AvgProfit\")\n",
    "\n",
    "    result = total_sales.join(avg_profit, \"Category\").select(\"Category\", \"sum(Sales)\", \"avg(Profit)\")\n",
    "\n",
    "    result = result.withColumnRenamed(\"sum(Sales)\", \"TotalSales\").withColumnRenamed(\"avg(Profit)\", \"AvgProfit\")\n",
    "\n",
    "    result.show()\n",
    "    return result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_lstm_model(data, feature_cols, label_col):\n",
    "\n",
    "    vector_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "    data = vector_assembler.transform(data)\n",
    "\n",
    "    train_data, test_data = data.randomSplit([0.8, 0.2], seed=123)\n",
    "\n",
    "  \n",
    "    lstm = LSTM(featuresCol=\"features\", labelCol=label_col, predictionCol=\"Prediction\",\n",
    "                maxIter=10, stepSize=0.1, inputSize=10, outputSize=1, blockSize=128)\n",
    "\n",
    "   \n",
    "    pipeline = Pipeline(stages=[lstm])\n",
    "    model = pipeline.fit(train_data)\n",
    "\n",
    " \n",
    "    predictions = model.transform(test_data)\n",
    "    evaluator = RegressionEvaluator(labelCol=label_col, predictionCol=\"Prediction\", metricName=\"rmse\")\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "file_path = r\"D:\\estudos_0012708\\dados_super_store.csv\"\n",
    "\n",
    "\n",
    "data = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "\n",
    "feature_cols = ['Quantity', 'Discount', 'Profit'] \n",
    "label_col = \"Sales\" \n",
    "\n",
    "\n",
    "trained_model = train_lstm_model(data, feature_cols, label_col)\n",
    "\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=r\"D:\\estudos_0012708\\dados_super_store.csv\"\n",
    "\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True,sep=\";\")\n",
    "df.columns\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
